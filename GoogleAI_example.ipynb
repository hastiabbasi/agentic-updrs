{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReAct agent from scratch with Gemini 2.5 and LangGraph\n",
    "[Google AI for Developers](https://ai.google.dev/gemini-api/docs/langgraph-example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "from langchain_core.messages import BaseMessage\n",
    "# helper function tp add messages to the state\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from geopy.geocoders import Nominatim\n",
    "from pydantic import BaseModel, Field\n",
    "import requests\n",
    "\n",
    "from datetime import datetime\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.runnables import RunnableConfig \n",
    "\n",
    "from langgraph.graph import StateGraph, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"the state of the agent\"\"\"\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    number_of_steps: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocator = Nominatim(user_agent=\"weather-app\")\n",
    "\n",
    "class SearchInput(BaseModel):\n",
    "    location:str = Field(description=\"They city and state, e.g., San Francisco\")\n",
    "    date:str = Field(description=\"The forecasting date for when to get the weather format (yyyy-mm-dd)\")\n",
    "\n",
    "@tool(\"get_weather_forecast\", args_schema=SearchInput, return_direct=True)\n",
    "def get_weather_forecast(location: str, date: str):\n",
    "    \"\"\"Retreives the weather using Open-Meteo API for a given location (city) and a date (yyyy-mm-dd). Returns a list dictionary with the time and temperature for each hour.\"\"\"\n",
    "    location = geolocator.geocode(location)\n",
    "\n",
    "    if location:\n",
    "        try:\n",
    "            response = requests.get(f\"https://api.open-meteo.com/v1/forecast?latitude={location.latitude}&longitude={location.longitude}&hourly=temperature_2m&start_date={date}&end_date={date}\")\n",
    "            data = response.json()\n",
    "            return {time: temp for time, temp in zip(data[\"hourly\"][\"time\"], data[\"hourly\"][\"temperature_2m\"])}\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "    else:\n",
    "        return {\"error\": \"Location not found\"}\n",
    "    \n",
    "tools = [get_weather_forecast]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"I'm sorry, but I can only provide the weather on an hourly basis.\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run--51f78b5a-c579-4af0-b6ec-a0c130a4583c-0' usage_metadata={'input_tokens': 149, 'output_tokens': 17, 'total_tokens': 435, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "# create LLM class\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model = \"gemini-2.5-pro\",\n",
    "    temperature = 1.0,\n",
    "    max_retries = 2,\n",
    "    #google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    ")\n",
    "\n",
    "# bind tools to the model \n",
    "model = llm.bind_tools([get_weather_forecast])\n",
    "\n",
    "# test the model with tools\n",
    "res = model.invoke(f\"What is the weather in Berlin on {datetime.today()}?\")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "# define tool node\n",
    "def call_tool(state: AgentState):\n",
    "    outputs = []\n",
    "    # iterate over the tool calls in the last message\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        # get the tool by name\n",
    "        tool_result = tools_by_name[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n",
    "        outputs.append(\n",
    "            ToolMessage(\n",
    "                content = tool_result, \n",
    "                name = tool_call[\"name\"],\n",
    "                tool_call_id = tool_call[\"id\"],\n",
    "            )\n",
    "        )\n",
    "    return {\"messages\": outputs}\n",
    "\n",
    "def call_model(\n",
    "        state: AgentState,\n",
    "        config: RunnableConfig,\n",
    "):\n",
    "    # invoke the model with the system prompt and the messages\n",
    "    response = model.invoke(state[\"messages\"], config)\n",
    "    # return a list because this will get added to the existing messages state using the add_messages reducer\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# define the conditional edge that determines whether to continue or not\n",
    "def should_continue(state: AgentState):\n",
    "    messages = state[\"messages\"]\n",
    "    # if the last message is not a tool call, then we finish\n",
    "    if not messages[-1].tool_calls:\n",
    "        return \"end\"\n",
    "    # default to continue \n",
    "    return \"continue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a new graph with state\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# add nodes\n",
    "workflow.add_node(\"llm\", call_model)\n",
    "workflow.add_node(\"tools\", call_tool)\n",
    "# set the entrypoint as agent (first node called)\n",
    "workflow.set_entry_point(\"llm\")\n",
    "# add a conditional edge after the llm node is called\n",
    "workflow.add_conditional_edges(\n",
    "    # edge is used after the llm node is called\n",
    "    \"llm\",\n",
    "    # function that will determine which node is called next\n",
    "    should_continue,\n",
    "    # mapping for where to go next, keys are strings from the funciton return, and the values are other nodes\n",
    "    # END is a special node marking that the graph is finished\n",
    "    {\n",
    "        # if tools, then we call the tool node\n",
    "        \"continue\": \"tools\",\n",
    "        # otherwise finish\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# add a normal edge after tools is called, llm node is called next\n",
    "workflow.add_edge(\"tools\", \"llm\")\n",
    "\n",
    "# compile and visiualize graph\n",
    "graph = workflow.compile()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
